
# HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models



![](./assets/teaser.png)

<p align="center">
  <a href='https://arxiv.org/abs/2312.06553'>
    <img src='https://img.shields.io/badge/Arxiv-2312.06553-A42C25?style=flat&logo=arXiv&logoColor=A42C25'>
  </a>
  <a href='https://arxiv.org/pdf/2312.06553.pdf'>
    <img src='https://img.shields.io/badge/Paper-PDF-yellow?style=flat&logo=arXiv&logoColor=yellow'>
  </a>
  <a href='https://neu-vi.github.io/HOI-Diff/'>
  <img src='https://img.shields.io/badge/Project-Page-orange?style=flat&logo=Google%20chrome&logoColor=orange'></a>
  <!-- <a href='https://youtu.be/0a0ZYJgzdWE'>
  <img src='https://img.shields.io/badge/YouTube-Video-EA3323?style=flat&logo=youtube&logoColor=EA3323'></a> -->
  <a href='https://github.com/neu-vi/HOI-Diff'>
    <img src='https://img.shields.io/badge/GitHub-Code-black?style=flat&logo=github&logoColor=white'></a>
  <a href="" target='_blank'>
    <img src="https://visitor-badge.laobi.icu/badge?page_id=neu-vi.HOI-Diff&left_color=gray&right_color=blue">
  </a>
</p>


<p align="center">
<!-- <h1 align="center">InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion</h1> -->
<strong>HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models</strong></h1>
   <p align="center">
    <a href='https://xiaogangpeng.github.io' target='_blank'>Xiaogang Peng*</a>&emsp;
    <a href='https://ymingxie.github.io' target='_blank'>Yiming Xie*</a>&emsp;
    <a href='http://zizhao.me/' target='_blank'>Zizhao Wu</a>&emsp;
    <a href='https://varunjampani.github.io/' target='_blank'>Varun Jampani</a>&emsp;
    <a href='https://deqings.github.io/' target='_blank'>Deqing Sun</a>&emsp;
    <a href='https://jianghz.me/' target='_blank'>Huaizu Jiang</a>&emsp;
    <br>
    Northeastern University &emsp; Hangzhou Dianzi University &emsp;
    Stability AI &emsp; Google Research
    <br>
    CVPR 2025 Workshop of HuMoGen
  </p>
</p>

## üíª Demo
![](./assets/demo.gif)


## üìú TODO List
- [x] Release the dataset preparation and annotations.
- [x] Release the main codes for implementation.
- [x] Release the pretrained models.
- [ ] Release the evaluation codes. (We are still working on developing a more robust evaluator.)
- [ ] Release the demo video.

## üì• Data Preparation

<!-- For more information about the implementation, see [README](utils/README.md).   -->
Please prepare and organize the data by following this [instruction](utils/README.md). 

## ‚öôÔ∏è Quick Start
<details>
  <summary><b>Setup and download</b></summary>



### 1. Setup environment
Install ffmpeg (if not already installed):

```
sudo apt update
sudo apt install ffmpeg
```

Update installed ffmpeg path in ./data_loaders/behave/utils/plot_script.py:
```
# Line 5
plt.rcParams['animation.ffmpeg_path'] = 'your_ffmpeg_path/bin/ffmpeg'
```

Setup conda env:
```
conda env create -f environment.yml
conda activate t2hoi

python -m spacy download en_core_web_sm
pip install git+https://github.com/openai/CLIP.git
```


Download dependencies:
```
bash prepare/download_smpl_files.sh
bash prepare/download_glove.sh
bash prepare/download_t2m_evaluators.sh  
```

Pleas follow [this](https://github.com/erikwijmans/Pointnet2_PyTorch) to install PointNet++.

### 2. Download Pre-trained model

`HOI-DM and APDM:` 
The [pretrained model](https://drive.google.com/file/d/1Xp78VJyEbWGN8nWD5T08KWiz9P4VLwci/view?usp=sharing) (including HOI-DM and APDM):  
```
gdown 1Xp78VJyEbWGN8nWD5T08KWiz9P4VLwci
unzip pretrain_model.zip -d .
rm pretrain_model.zip
```

<!-- HOI-DM's pretrained [checkpoint](https://drive.google.com/drive/folders/1WQaaqQumkzm_c1zJ_UfmfSSOIC_h4w8D?usp=drive_link) and APDM's pretrained [checkpoint](https://drive.google.com/drive/folders/1ynFIvZXiopuHGOxuLkSsyvNMMCs9zHxr?usp=drive_link). Please place them in the ‚Äúsave‚Äù folder and update the parser file so it loads them correctly. -->


### 3. HOIs Synthesis with pretrained model
Generate from test set prompts
```
python -m sample.local_generate_obj --model_path ./save/behave_enc_512/model000020000.pt --num_samples 10 --num_repetitions 1 --motion_length 10 --multi_backbone_split 4 --guidance
```
Generate from your text file
```
python -m sample.local_generate_obj --model_path ./save/behave_enc_512/model000020000.pt --num_samples 10 --num_repetitions 1 --motion_length 10 --multi_backbone_split 4 --guidance --input_text ./assets/your_text.txt
```

### 4. Train your APDM
```
python -m train.train_affordance --save_dir ./save/my_afford_pred --dataset behave --save_interval 1000 --num_steps 20000 --batch_size 32 --diffusion_steps 500
```

### 5. Train your HOI-DM
`MDM:` Before your training, please download the pre-trained model [here](https://drive.google.com/file/d/1PE0PK8e5a5j-7-Xhs5YET5U5pGh0c821/view?pli=1), then unzip and place them in ./checkpoints/.
```
mkdirs checkpoints
cd checkpoints
gdown 1PE0PK8e5a5j-7-Xhs5YET5U5pGh0c821
unzip humanml_trans_enc_512.zip -d .
cd ..
```
```
python -m train.hoi_diff --save_dir ./save/my_behave_enc_512 --dataset behave --save_interval 1000 --num_steps 20000 --arch trans_enc --batch_size 32
```


<!-- ### 6. Evaluate
```
python -m eval.eval_behave --model_path ./save/behave_enc_512/model000020000.pt  --guidance --comment eval_behave
``` -->

</details>

## Visualization
<details>
<summary><b> Render SMPL mesh</b></summary>

To create SMPL mesh per frame run:

```shell
python -m visualize.render_mesh --input_path /path/to/mp4/stick/figure/file
```

**This script outputs: [YOUR_NPY_FOLDER]**
* `sample##_rep##_smpl_params.npy` - SMPL parameters (human_motion, thetas, root translations, human_vertices and human_faces)
* `sample##_rep##_obj_params.npy` - SMPL parameters (object_motion, object_vertices and object_faces)

**Notes:**
* This script is running [SMPLify](https://smplify.is.tue.mpg.de/) and needs GPU as well (can be specified with the `--device` flag).
* **Important** - Do not change the original `.mp4` path before running the script.

### 1. Set up blender - WIP

Refer to [TEMOS-Rendering motions](https://github.com/Mathux/TEMOS) for blender setup, then install the following dependencies.

```
YOUR_BLENDER_PYTHON_PATH/python -m pip install -r prepare/requirements_render.txt
```
### 2. Render SMPL meshes

Run the following command to render SMPL using blender:

```
YOUR_BLENDER_PATH/blender --background --python render.py -- --cfg=./configs/render.yaml --dir=YOUR_NPY_FOLDER --mode=video --joint_type=HumanML3D
```

optional parameters:

- `--mode=video`: render mp4 video
- `--mode=sequence`: render the whole motion in a png image.
</details>


## Acknowledgments
This code is standing on the shoulders of giants. We want to thank the following contributors that our code is based on:

[BEHAVE](https://github.com/xiexh20/behave-dataset), [MLD](https://github.com/ChenFengYe/motion-latent-diffusion), [MDM](https://github.com/GuyTevet/motion-diffusion-model), [GMD](https://github.com/korrawe/guided-motion-diffusion), [guided-diffusion](https://github.com/openai/guided-diffusion), [MotionCLIP](https://github.com/GuyTevet/MotionCLIP), [text-to-motion](https://github.com/EricGuo5513/text-to-motion), [actor](https://github.com/Mathux/ACTOR), [joints2smpl](https://github.com/wangsen1312/joints2smpl), [MoDi](https://github.com/sigal-raab/MoDi).

## ü§ù Citation
If you find this repository useful for your work, please consider citing it as follows:
```bibtex
@article{peng2023hoi,
  title={HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models},
  author={Peng, Xiaogang and Xie, Yiming and Wu, Zizhao and Jampani, Varun and Sun, Deqing and Jiang, Huaizu},
  journal={arXiv preprint arXiv:2312.06553},
  year={2023}
}
```
